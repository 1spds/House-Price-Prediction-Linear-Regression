# California Housing Price Prediction using Linear Regression

## Objective:
To build and evaluate a Linear Regression model that predicts median house prices in California districts based on various socio-economic and geographic features.

## Introduction

In this project, we aim to **predict housing prices in California** using **Linear Regression**, a fundamental machine learning algorithm for regression problems. The dataset contains various housing-related features such as location (latitude and longitude), total rooms, population, median income, and the median house value (our target variable).

### Project Workflow:

1. **Objective**  
   We define the goal of the project: to build a predictive model for estimating housing prices based on available features.

2. **About this File**  
   A brief overview of the dataset, which includes housing data for California from the 1990 U.S. Census.

3. **Exploratory Data Analysis (EDA)**  
   We explore the dataset to understand its structure, identify patterns, correlations, and gain insights into the features that may influence house prices.

4. **Preprocessing**  
   This step involves selecting features and the target variable, handling missing values, and splitting the data into training and testing sets.

5. **Model Building**  
   A Linear Regression model is trained using the training dataset to learn the relationships between features and house values.

6. **Model Evaluation**  
   We evaluate the model's performance using metrics like MAE, MSE, and RMSE. Additionally, we visualize the results through:
   - **Actual vs Predicted Plot** to assess model accuracy  
   - **Residual Plot** to detect bias or patterns in errors

7. **Deployment**  
   The trained model is saved and later reloaded to make predictions on new data, simulating how the model could be used in a real-world application.

8. **Conclusion**  
   We summarize the model’s performance, key findings, and provide suggestions for future improvements like using advanced models or adding more informative features.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error,root_mean_squared_error

## The data set
df = pd.read_csv('housing.csv')
df
## About this file

1. longitude: A measure of how far west a house is; a higher value is farther west

2. latitude: A measure of how far north a house is; a higher value is farther north

3. housingMedianAge: Median age of a house within a block; a lower number is a newer building

4. totalRooms: Total number of rooms within a block

5. totalBedrooms: Total number of bedrooms within a block

6. population: Total number of people residing within a block

7. households: Total number of households, a group of people residing within a home unit, for a block

8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)

9. medianHouseValue: Median house value for households within a block (measured in US Dollars)

10. oceanProximity: Location of the house w.r.t ocean/sea
## Exploratory Data Analysis (EDA)
df.shape

df.info()

df.describe()
df.isnull()
plt.figure(figsize=(15, 6), dpi=200)
sns.histplot(data=df, x='median_house_value', bins=100, kde=True)
## Preprocessing
### Define features and label:
X = df.drop(['median_house_value', 'ocean_proximity'], axis=1)
X = X.fillna(X.median())
X

y = df['median_house_value']
y
### Train-test split:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
## Model Building
### Fit Linear Regression:
model = LinearRegression()

model.fit(X_train, y_train)
### Predictions:
test_predictions = model.predict(X_test)
test_predictions
## Model Evaluation
print(f"the MAE is {mean_absolute_error(y_test, test_predictions)}")
print(f"the MSE is {mean_squared_error(y_test, test_predictions)}")
print(f"the RMSE is {root_mean_squared_error(y_test, test_predictions)}")
### Actual vs Predicted plot:
plt.figure(figsize=(15, 6), dpi=200)
sns.scatterplot(x=y_test, y=test_predictions)
plt.xlabel("Actual House Value")
plt.ylabel("Predicted House Value")
plt.title("Actual vs Predicted Values")
plt.show()

### Residual Plot
test_residuals = y_test - test_predictions
test_residuals
plt.figure(figsize=(15, 6), dpi=200)
sns.scatterplot(x=y_test, y= test_residuals)
plt.axhline(y=0, color='red', ls='--')
## Deployment
final_model = LinearRegression()
final_model.fit(X,y)
final_model.coef_
y_hat = final_model.predict(X)

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))
fig.suptitle("Feature-wise Prediction vs Actual House Value", fontsize=16, y=1.02)

# Common settings
features = ['median_income', 'housing_median_age', 'total_rooms', 'total_bedrooms']
titles = ['Median Income', 'Housing Median Age', 'Total Number of Rooms', 'Total Number of Bedrooms']
y_max = 600000

for ax, feature, title in zip(axes.flat, features, titles):
    ax.scatter(df[feature], df['median_house_value'], label='Actual', alpha=0.4)
    ax.scatter(df[feature], y_hat, color='red', label='Predicted', alpha=0.4)
    
    ax.set_title(title, fontsize=14)
    ax.set_xlabel(feature.replace('_', ' ').title(), fontsize=12)
    ax.set_ylabel("Median House Value", fontsize=12)
    ax.set_ylim(0, y_max)
    ax.grid(True)
    ax.legend()

plt.tight_layout()
plt.show()
## Saving the model
from joblib import dump, load
dump(final_model, 'final_houseing_model.joblib')
## Loading the model
loaded_model = load('final_sales_model.joblib')
loaded_model.coef_
## Predicting using this model
new = [[-122.23, 37.88, 41.0, 880.0, 129.0, 322.0, 126.0, 8.3252]]

loaded_model.predict(new)
# Conclusion
In this project, we built a linear regression model to predict median house values in California using features such as median income, location, and housing characteristics.

## Model Evaluation Results:
### Mean Absolute Error (MAE): $51,049

### Mean Squared Error (MSE): 4.86 × 10⁹

### Root Mean Squared Error (RMSE): $69,692

## Interpretation:
The MAE of $51,049 implies that, on average, our model’s predictions are off by about $51k.

The RMSE gives more weight to larger errors and is around $69k, which is reasonable considering California housing prices.

The error values suggest that while the model captures the general trend in house prices, there is still significant room for improvement.

## Key Observations:
Median income has a strong correlation with house value and significantly influenced predictions.

Features like total rooms, bedrooms, and housing age also contributed but with lower impact.

Some prediction errors may stem from high variance or skewness in housing values and possible outliers.

## Possible Improvements:
### Use feature engineering (e.g., rooms per household).

### Try non-linear models (e.g., decision trees, random forest, gradient boosting).

### Perform outlier detection and scaling for better model generalization.

### Include ocean_proximity after encoding it properly as a categorical variable.
